{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc57505-b5fb-4fc4-b7f6-a25875f342e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Starter code for the problem \"Widget sales\".\n",
    "\n",
    "Autonomous Systems Lab (ASL), Stanford University\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Seed RNG for reproducibility\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Define the state space, action space, and demand distribution\n",
    "S = np.array([0, 1, 2, 3, 4, 5])\n",
    "A = np.array([0, 2, 4])\n",
    "D = np.array([0, 1, 2, 3, 4])\n",
    "P = np.array([0.1, 0.3, 0.3, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a39b1c-9992-4290-89f6-bf419a470900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4639a53e8e04470aebf189906bf9212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1095 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transition(s: int, a: int, d: int) -> int:\n",
    "    \"\"\"Compute the next state given the current state, action, and demand.\"\"\"\n",
    "    s_next = np.clip(s + a - d, 0, 5)\n",
    "    return s_next\n",
    "\n",
    "\n",
    "def reward(s: int, a: int, d: int) -> float:\n",
    "    \"\"\"Compute the reward given the current state, action, and demand.\"\"\"\n",
    "    price = 1.2\n",
    "    cost_rent = 1.0\n",
    "    cost_storage = 0.05 * s\n",
    "    cost_order = np.sqrt(a)\n",
    "    r = price * np.minimum(s + a, d) - cost_rent - cost_storage - cost_order\n",
    "    return r\n",
    "\n",
    "\n",
    "def simulate(\n",
    "    rng: np.random.Generator,\n",
    "    policy: callable,\n",
    "    T: int,\n",
    "    s0: int = 5,\n",
    "    D: np.ndarray = D,\n",
    "    P: np.ndarray = P,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Simulate widget sales for a given policy.\"\"\"\n",
    "    s = np.zeros(T + 1)  # states\n",
    "    a = np.zeros(T)  # actions\n",
    "    r = np.zeros(T)  # rewards\n",
    "    s[0] = s0  # initial state\n",
    "\n",
    "    for t in tqdm(range(T)):\n",
    "        # Sample demand\n",
    "        d = rng.choice(D, p=P)\n",
    "\n",
    "        # Record action, reward, and next state\n",
    "        a[t] = policy(s[t])\n",
    "        r[t] = reward(s[t], a[t], d)\n",
    "        s[t + 1] = transition(s[t], a[t], d)\n",
    "\n",
    "    return s, a, r\n",
    "\n",
    "\n",
    "# Generate historical data with a uniformly random policy\n",
    "log = {}\n",
    "T = 3 * 365\n",
    "log[\"s\"], log[\"a\"], log[\"r\"] = simulate(rng, lambda s, A=A: rng.choice(A), T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad4932-76f1-429e-801d-765e1f919d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Q-learning\n",
    "γ = 0.95  # discount factor\n",
    "α = 1e-2  # learning rate\n",
    "num_epochs = 5 * int(1 / α)  # number of epochs\n",
    "\n",
    "Q = np.zeros((S.size, A.size))\n",
    "Q_epoch = np.zeros((num_epochs + 1, S.size, A.size))\n",
    "\n",
    "for k in tqdm(range(1, num_epochs + 1)):\n",
    "    # Shuffle transition tuple indices\n",
    "    shuffled_indices = rng.permutation(T)\n",
    "\n",
    "    # ####################### PART (a): YOUR CODE BELOW #######################\n",
    "\n",
    "    # INSTRUCTIONS: Update `Q` using Q-learning.\n",
    "\n",
    "    # ############################# END PART (a) ##############################\n",
    "\n",
    "    # Record Q-values for this epoch\n",
    "    Q_epoch[k] = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c42c43-05c3-40d4-aabd-4a4da8ff5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do value iteration\n",
    "converged = False\n",
    "eps = 1e-4\n",
    "max_iters = 500\n",
    "Q_vi = np.zeros((S.size, A.size))\n",
    "Q_vi_prev = np.full(Q_vi.shape, np.inf)\n",
    "\n",
    "for k in tqdm(range(max_iters)):\n",
    "\n",
    "    # ####################### PART (b): YOUR CODE BELOW #######################\n",
    "\n",
    "    # INSTRUCTIONS: Update `Q_vi` using value iteration.\n",
    "\n",
    "    # ############################# END PART (b) ##############################\n",
    "\n",
    "    if np.max(np.abs(Q_vi - Q_vi_prev)) < eps:\n",
    "        converged = True\n",
    "        print(\"Value iteration converged after {} iterations.\".format(k))\n",
    "        break\n",
    "    else:\n",
    "        np.copyto(Q_vi_prev, Q_vi)\n",
    "\n",
    "if not converged:\n",
    "    raise RuntimeError(\"Value iteration did not converge!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152b7d4-fc65-4dd0-9d00-d80be2fa73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Q-values for each epoch\n",
    "fig, axes = plt.subplots(\n",
    "    2, S.size // 2, figsize=(12, 6), sharex=True, sharey=True, dpi=150\n",
    ")\n",
    "fig.subplots_adjust(hspace=0.2)\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    for j in range(A.size):\n",
    "        plot = ax.plot(Q_epoch[:, i, j], label=\"$a = {}$\".format(A[j]))\n",
    "        ax.axhline(Q_vi[i, j], linestyle=\"--\", color=plot[0].get_color())\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.set_title(r\"$s = {}$\".format(S[i]))\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel(\"$Q(s,a)$\")\n",
    "fig.savefig(\"widget_sales_qvalues.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d63fb8-531e-4ffc-a75c-73c0d6a68bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report optimal policies from Q-learning and value iteration, simulate them\n",
    "# for 5 years, and plot the cumulative profits\n",
    "\n",
    "# ######################### PART (c): YOUR CODE BELOW #########################\n",
    "\n",
    "# INSTRUCTIONS: Compute the optimal actions `a_opt_ql` and `a_opt_vi` using the\n",
    "#               Q-values from Q-learning and value iteration, respectively.\n",
    "#               Both `a_opt_ql` and `a_opt_vi` should be `np.ndarray`s, where\n",
    "#               each entry is the optimal action for the corresponding state.\n",
    "#\n",
    "#               Also, simulate each optimal policy and compute the history of\n",
    "#               cumulative profits `profit_ql` and `profit_vi` over 5 years\n",
    "#               (at 365 days per year).\n",
    "\n",
    "T = 5 * 365\n",
    "\n",
    "# TODO: replace the next four lines with your code\n",
    "a_opt_ql = np.zeros(S.size)\n",
    "profit_ql = np.zeros(T)\n",
    "a_opt_vi = np.zeros(S.size)\n",
    "profit_vi = np.zeros(T)\n",
    "\n",
    "# ############################### END PART (c) ################################\n",
    "\n",
    "print(\"Optimal policy (Q-learning):     \", a_opt_ql)\n",
    "print(\"Optimal policy (value iteration):\", a_opt_vi)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(profit_ql, label=r\"$Q$-learning\")\n",
    "ax.plot(profit_vi, label=r\"value iteration\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlabel(r\"day $t$\")\n",
    "ax.set_ylabel(r\"cumulative profit $\\sum_{k=0}^t r_k$\")\n",
    "fig.savefig(\"widget_sales_profits.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
